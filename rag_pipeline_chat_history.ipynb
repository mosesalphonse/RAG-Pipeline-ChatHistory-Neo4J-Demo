# -------------------------------------------------
# Cell 1: Install correct packages
# -------------------------------------------------
!pip install -qU \
  "langchain-groq>=0.2.0" \
  "langchain>=0.2.0" \
  "langchain-community>=0.2.0" \
  "langchain-core>=0.2.0" \
  "langchain-text-splitters>=0.2.0" \
  neo4j \
  pypdf \
  sentence-transformers
  
  # -------------------------------------------------
# Cell 2: Imports + Secrets + Tiny Model
# -------------------------------------------------
import os
from google.colab import userdata, files
from datetime import datetime, timedelta
from typing import List, Dict

# LangChain
from langchain_groq import ChatGroq
from langchain_community.vectorstores import Neo4jVector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from neo4j import GraphDatabase
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# === Secrets ===
try:
    GROQ_API_KEY   = userdata.get('GROQ_API_KEY')
    NEO4J_URI      = userdata.get('NEO4J_URI')
    NEO4J_USERNAME = userdata.get('NEO4J_USERNAME')
    NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')
except Exception as e:
    raise RuntimeError(f"Add secret: {e}")

# === Tiny embedding (30 MB) ===
print("Loading embedding model...")
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
print("Embedding ready.")

# === Tiny LLM (2B) ===
llm = ChatGroq(model_name="llama-3.1-8b-instant", groq_api_key=GROQ_API_KEY, temperature=0.1)

# === Neo4j ===
driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))

# === Constants ===
VECTOR_DIM         = 384
CHUNK_INDEX_NAME   = "chunk_embeddings"
HISTORY_INDEX_NAME = "history_embeddings"
USER_ID            = "demo_user"
SESSION_ID         = "light_session"
RETENTION_DAYS     = 90
MAX_CHUNKS         = 12

# -------------------------------------------------
# Cell 3: Upload tiny file (PDF/TXT)
# -------------------------------------------------
print("Upload a small PDF or TXT (≤ 2 pages)")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"Uploaded: {filename}")

# Load first 2 pages only
if filename.lower().endswith(".pdf"):
    loader = PyPDFLoader(filename)
    pages = loader.load()[:2]
elif filename.lower().endswith(".txt"):
    loader = TextLoader(filename, encoding="utf-8")
    pages = loader.load()
else:
    raise ValueError("Use .pdf or .txt")

print(f"Loaded {len(pages)} page(s).")

# -------------------------------------------------
# Cell 4: Split & Ingest (CORRECTED)
# -------------------------------------------------
splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
chunks = splitter.split_documents(pages)[:MAX_CHUNKS]
print(f"Using {len(chunks)} chunks.")

# FIXED: No `text_node_properties`
Neo4jVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
    index_name=CHUNK_INDEX_NAME,
    node_label="Chunk",
    embedding_node_property="embedding"
    # → text is stored in default `text` property
)
print("Ingestion complete.")

# -------------------------------------------------
# Cell 5: Light Chat History
# -------------------------------------------------
class LightChatHistory:
    def __init__(self, driver):
        self.driver = driver

    def add(self, role: str, content: str):
        ts = datetime.now().isoformat()
        emb = embeddings.embed_query(content)
        with self.driver.session() as s:
            s.run(
                """
                MERGE (u:User {user_id: $uid})
                MERGE (sess:Session {session_id: $sid})
                MERGE (u)-[:HAS_SESSION]->(sess)
                CREATE (m:Message {role: $role, content: $content,
                                  ts: datetime($ts), emb: $emb})
                CREATE (sess)-[:HAS]->(m)
                """,
                uid=USER_ID, sid=SESSION_ID, role=role,
                content=content, ts=ts, emb=emb
            )

    def get(self, query: str, k: int = 2) -> List[Dict]:
        q_emb = embeddings.embed_query(query)
        with self.driver.session() as s:
            res = s.run(
                """
                CALL db.index.vector.queryNodes($idx, $top, $q_emb)
                YIELD node, score
                WHERE node.ts > datetime() - duration({days: $ret})
                RETURN node.role AS role, node.content AS content
                ORDER BY score DESC LIMIT $k
                """,
                idx=HISTORY_INDEX_NAME, top=k+3, q_emb=q_emb,
                ret=RETENTION_DAYS, k=k
            )
            return [dict(r) for r in res]

    def cleanup(self):
        cutoff = (datetime.now() - timedelta(days=RETENTION_DAYS)).isoformat()
        with self.driver.session() as s:
            s.run("MATCH (m:Message) WHERE m.ts < datetime($c) DETACH DELETE m", c=cutoff)

# Create history index
with driver.session() as s:
    s.run(
        f"""
        CREATE VECTOR INDEX `{HISTORY_INDEX_NAME}` IF NOT EXISTS
        FOR (m:Message) ON (m.emb)
        OPTIONS {{indexConfig: {{`vector.dimensions`: {VECTOR_DIM}, `vector.similarity_function`: 'cosine'}}}}
        """
    )
print("History index ready.")

history = LightChatHistory(driver)



# -------------------------------------------------
# Cell 6: RAG Chain (light)
# -------------------------------------------------
vector_store = Neo4jVector.from_existing_index(
    embedding=embeddings,
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
    index_name=CHUNK_INDEX_NAME
)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

prompt = ChatPromptTemplate.from_messages([
    ("system",
     "Answer briefly using knowledge and past chat.\n"
     "Knowledge: {context}\n"
     "Past: {history}\n"
     "Q: {question}\nAnswer:"),
    MessagesPlaceholder("chat_history"),
    ("human", "{question}")
])

def ask(query: str) -> str:
    past = history.get(query, k=2)
    hist_txt = "\n".join([f"{p['role']}: {p['content']}" for p in past])
    chat_msgs = [
        HumanMessage(content=p['content']) if p['role']=='human'
        else AIMessage(content=p['content'])
        for p in past
    ]

    docs = retriever.invoke(query)
    context = " | ".join([d.page_content[:180] for d in docs])

    chain = prompt | llm | StrOutputParser()
    ans = chain.invoke({
        "context": context,
        "history": hist_txt,
        "question": query,
        "chat_history": chat_msgs
    })

    history.add("human", query)
    history.add("assistant", ans)
    return ans
	
# -------------------------------------------------
# Cell 7: Chat
# -------------------------------------------------
print("\nLIGHT RAG CHAT (2B LLM, ≤12 chunks)")
print("="*50)
print("Ask about your file. Try follow-ups!")
print("Commands: exit | cleanup\n")

while True:
    q = input("You: ").strip()
    if q.lower() == "exit":
        print("Bye!")
        break
    if q.lower() == "cleanup":
        history.cleanup()
        print("History cleaned.")
        continue
    if not q: continue

    print("Assistant:", ask(q))
    print()
